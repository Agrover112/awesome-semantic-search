# Busca SemÃ¢ntica IncrÃ­vel [![IncrÃ­vel](https://awesome.re/badge.svg)](https://awesome.re) [![Commits Convencionais](https://img.shields.io/badge/Commits%20Convencionais-1.0.0-amarelo.svg)](https://conventionalcommits.org)

<img src="logo.svg" />

Logo feito por [@createdbytango](https://instagram.com/createdbytango).

**Ã€ procura de mais adiÃ§Ãµes de artigos.
PS: Abra um PR (Pedido de Pull)**

Este repositÃ³rio visa servir como um meta-repositÃ³rio para tarefas relacionadas com [Busca SemÃ¢ntica](https://pt.wikipedia.org/wiki/Busca_semÃ¢ntica) e [Similaridade SemÃ¢ntica](http://nlpprogress.com/english/semantic_textual_similarity.html).

A busca semÃ¢ntica nÃ£o se limita a texto! Pode ser feito com imagens, voz, etc. Existem inÃºmeros casos de uso e diferentes aplicaÃ§Ãµes de busca semÃ¢ntica.

Sinta-se Ã  vontade para abrir um PR neste repositÃ³rio!

## ConteÃºdo

-   [Artigos](#artigos)
    -   [2014](#2014)
    -   [2015](#2015)
    -   [2016](#2016)
    -   [2017](#2017)
    -   [2018](#2018)
    -   [2019](#2019)
    -   [2020](#2020)
    -   [2021](#2021)
    -   [2022](#2022)
    -   [2023](#2023)
-   [Artigos](#artigos)
-   [Bibliotecas e Ferramentas](#bibliotecas-e-ferramentas)
-   [Conjuntos de Dados](#conjuntos-de-dados)
-   [Marcos](#marcos)

## Artigos

### 2010

-   [Priority Range Trees](https://arxiv.org/abs/1009.3527)

### 2014

-   [Um Modelo SemÃ¢ntico Latente com Estrutura de Convolutional-Pooling para RecuperaÃ§Ã£o de InformaÃ§Ã£o](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/cikm2014_cdssm_final.pdf) ğŸ“„

### 2015

-   [Vetores de Skip-Thought](https://arxiv.org/pdf/1506.06726.pdf) ğŸ“„
-   [LSH PrÃ¡tico e Ã“timo para DistÃ¢ncia Angular](https://proceedings.neurips.cc/paper/2015/hash/2823f4797102ce1a1aec05359cc16dd9-Abstract.html)

### 2016

-   [Saco de truques para classificaÃ§Ã£o eficiente de texto](https://arxiv.org/abs/1607.01759) ğŸ“„
-   [Enriquecendo vetores de palavras com informaÃ§Ãµes de subpalavras](https://arxiv.org/abs/1607.04606) ğŸ“„
-   [Pesquisa aproximada de vizinho mais prÃ³ximo eficiente e robusta usando grÃ¡ficos hierÃ¡rquicos navegÃ¡veis â€‹â€‹de pequenos mundos](https://arxiv.org/abs/1603.09320)
-   [Sobre a pesquisa aproximada de incorporaÃ§Ãµes de palavras semelhantes](https://www.aclweb.org/anthology/P16-1214.pdf)
-   [Aprendendo representaÃ§Ãµes distribuÃ­das de sentenÃ§as a partir de dados nÃ£o rotulados](https://arxiv.org/abs/1602.03483)ğŸ“„
-   [Pesquisa aproximada do vizinho mais prÃ³ximo em dados de alta dimensÃ£o --- Experimentos, anÃ¡lises e melhorias](https://arxiv.org/abs/1610.02455)

### 2017

-   [Aprendizagem supervisionada de representaÃ§Ãµes de frases universais a partir de dados de inferÃªncia de linguagem natural](https://research.fb.com/wp-content/uploads/2017/09/emnlp2017.pdf) ğŸ“„
-   [SemelhanÃ§a textual semÃ¢ntica para hindi] (https://www.semanticscholar.org/paper/Semantic-Textual-Similarity-For-Hindi-Mujadia-Mamidi/372f615ce36d7543512b8e40d6de51d17f316e0b)ğŸ“„
-   [SugestÃ£o eficiente de resposta em linguagem natural para resposta inteligente](https://arxiv.org/abs/1705.00652)ğŸ“ƒ

### 2018

-   [Codificador de frases universais](https://arxiv.org/pdf/1803.11175.pdf) ğŸ“„
-   [Aprendendo similaridade textual semÃ¢ntica em conversas](https://arxiv.org/pdf/1804.07754.pdf) ğŸ“„
-   [Blog de IA do Google: avanÃ§os na similaridade textual semÃ¢ntica](https://ai.googleblog.com/2018/05/advances-in-semantic-textual-similarity.html) ğŸ“„
-   [Speech2Vec: uma estrutura de sequÃªncia a sequÃªncia para aprender incorporaÃ§Ãµes de palavras a partir da fala](https://arxiv.org/abs/1803.08976))ğŸ”Š
-   [OtimizaÃ§Ã£o da indexaÃ§Ã£o com base no grÃ¡fico k-vizinho mais prÃ³ximo para pesquisa de proximidade em dados de alta dimensÃ£o](https://arxiv.org/abs/1810.07355) ğŸ”Š
-   [Pesquisa rÃ¡pida aproximada do vizinho mais prÃ³ximo com o
    Navegando no grÃ¡fico de dispersÃ£o](http://www.vldb.org/pvldb/vol12/p461-fu.pdf)
-   [O caso das estruturas de Ã­ndice aprendidas](https://dl.acm.org/doi/10.1145/3183713.3196909)

### 2019

-   [LASER: representaÃ§Ãµes de frases agnÃ³sticas de linguagem](https://engineering.fb.com/2019/01/22/ai-research/laser-multilingual-sentence-embeddings/) ğŸ“„
-   [ExpansÃ£o de documentos por previsÃ£o de consulta](https://arxiv.org/abs/1904.08375) ğŸ“„
-   [Sentence-BERT: Embeddings de frases usando redes BERT siamesas](https://arxiv.org/pdf/1908.10084.pdf) ğŸ“„
-   [ClassificaÃ§Ã£o de documentos em vÃ¡rios estÃ¡gios com BERT](https://arxiv.org/abs/1910.14424) ğŸ“„
-   [RecuperaÃ§Ã£o latente para resposta a perguntas de domÃ­nio aberto com supervisÃ£o fraca](https://arxiv.org/abs/1906.00300)
-   [Resposta completa de perguntas de domÃ­nio aberto com BERTserini](https://www.aclweb.org/anthology/N19-4013/)
-   [BioBERT: um modelo de representaÃ§Ã£o de linguagem biomÃ©dica prÃ©-treinado para mineraÃ§Ã£o de texto biomÃ©dico](https://arxiv.org/abs/1901.08746)ğŸ“„
-   [Analisando e melhorando representaÃ§Ãµes com a perda suave do vizinho mais prÃ³ximo](https://arxiv.org/pdf/1902.01889.pdf)ğŸ“·
-   [DiskANN: rÃ¡pido e preciso bilhÃ£o de pontos mais prÃ³ximo
    Pesquisa de vizinho em um Ãºnico nÃ³](https://proceedings.neurips.cc/paper/2019/file/09853c7fb1d3f8ee67a61b6bf4a7f8e6-Paper.pdf)

### 2020

-   [Implantando rapidamente um mecanismo de pesquisa neural para o conjunto de dados de pesquisa aberta COVID-19: reflexÃµes preliminares e liÃ§Ãµes aprendidas](https://arxiv.org/abs/2004.05125) ğŸ“„
-   [RE-RANKING DA PASSAGEM COM BERT](https://arxiv.org/pdf/1901.04085.pdf) ğŸ“„
-   [CO-Search: recuperaÃ§Ã£o de informaÃ§Ãµes sobre COVID-19 com pesquisa semÃ¢ntica, resposta a perguntas e resumo abstrativo](https://arxiv.org/pdf/2006.09595.pdf) ğŸ“„
-   [LaBSE: IncorporaÃ§Ã£o de frase BERT independente de idioma](https://arxiv.org/abs/2007.01852) ğŸ“„
-   [Covidex: Modelos de classificaÃ§Ã£o neural e infraestrutura de pesquisa de palavras-chave para o conjunto de dados de pesquisa aberta COVID-19](https://arxiv.org/abs/2007.07846) ğŸ“„
-   [DeText: uma estrutura profunda de PNL para compreensÃ£o inteligente de texto](https://engineering.linkedin.com/blog/2020/open-sourcing-detext) ğŸ“„
-   [Fazendo incorporaÃ§Ãµes de frases monolÃ­ngues multilÃ­ngues usando destilaÃ§Ã£o de conhecimento](https://arxiv.org/pdf/2004.09813.pdf) ğŸ“„
-   [Transformadores prÃ©-treinados para classificaÃ§Ã£o de texto: BERT e alÃ©m](https://arxiv.org/abs/2010.06467) ğŸ“„
-   [REALM: PrÃ©-treinamento de modelo de linguagem aumentada de recuperaÃ§Ã£o](https://arxiv.org/abs/2002.08909)
-   [ELECTRA: CODIFICADORES DE TEXTO DE PRÃ‰-TREINAMENTO COMO DISCRIMINADORES EM VEZ DE GERADORES](https://openreview.net/pdf?id=r1xMH1BtvB)ğŸ“„
-   [Melhorando o aprendizado profundo para pesquisa no Airbnb](https://arxiv.org/pdf/2002.05515)
-   [Gerenciando a Diversidade na Pesquisa Airbnb](https://arxiv.org/abs/2004.02621)ğŸ“„
-   [Aprendizagem contrastiva negativa aproximada do vizinho mais prÃ³ximo para recuperaÃ§Ã£o de texto denso](https://arxiv.org/abs/2007.00808v1)ğŸ“„
-   [IncorporaÃ§Ãµes de estilo de imagem nÃ£o supervisionado para tarefas de recuperaÃ§Ã£o e reconhecimento](https://openaccess.thecvf.com/content_WACV_2020/papers/Gairola_Unsupervised_Image_Style_Embeddings_for_Retrieval_and_Recognition_Tasks_WACV_2020_paper.pdf)ğŸ“·
-   [DeCLUTR: Aprendizagem Contrastiva Profunda para RepresentaÃ§Ãµes Textuais NÃ£o Supervisionadas](https://arxiv.org/abs/2006.03659)ğŸ“„

### 2021

-   [Abordagem hÃ­brida para cÃ¡lculo de similaridade semÃ¢ntica entre palavras Tamil](https://www.researchgate.net/publication/350112163_Hybrid_approach_for_semantic_similarity_calculation_between_Tamil_words) ğŸ“„
-   [SBERT aumentado](https://arxiv.org/pdf/2010.08240.pdf) ğŸ“„
-   [BEIR: um benchmark heterogÃªneo para avaliaÃ§Ã£o zero-shot de modelos de recuperaÃ§Ã£o de informaÃ§Ãµes](https://arxiv.org/abs/2104.08663) ğŸ“„
-   [Pesquisa visual heterogÃªnea com reconhecimento de compatibilidade](https://arxiv.org/abs/2105.06047) ğŸ“·
-   [Aprendendo estilo pessoal com alguns exemplos](https://chuanenlin.com/personalstyle)ğŸ“·
-   [TSDAE: Usando codificador automÃ¡tico de eliminaÃ§Ã£o de ruÃ­do sequencial baseado em transformador para aprendizagem nÃ£o supervisionada de incorporaÃ§Ã£o de frases](https://arxiv.org/abs/2104.06979)ğŸ“„
-   [Uma Pesquisa de Transformadores](https://arxiv.org/abs/2106.04554)ğŸ“„ğŸ“·
-   [SPLADE: modelo lexical esparso e de expansÃ£o para classificaÃ§Ã£o de primeiro estÃ¡gio](https://dl.acm.org/doi/10.1145/3404835.3463098)ğŸ“„
-   [SugestÃµes de consulta de pesquisa relacionada de alta qualidade usando Deep Reinforcement Learning](https://arxiv.org/abs/2108.04452v1)
-   [RecuperaÃ§Ã£o de produto baseada em incorporaÃ§Ã£o na pesquisa Taobao](https://arxiv.org/pdf/2106.09297.pdf)ğŸ“„ğŸ“·
-   [TPRM: um modelo de classificaÃ§Ã£o personalizado baseado em tÃ³picos para pesquisa na Web](https://arxiv.org/abs/2108.06014)ğŸ“„
-   [mMARCO: uma versÃ£o multilÃ­ngue do conjunto de dados de classificaÃ§Ã£o de passagens MS MARCO](https://arxiv.org/abs/2108.13897)ğŸ“„
-   [RaciocÃ­nio de banco de dados sobre texto](https://aclanthology.org/2021.acl-long.241.pdf)ğŸ“„
-   [Como o ajuste fino adversÃ¡rio beneficia o BERT?](https://arxiv.org/abs/2108.13602))ğŸ“„
-   [Treinar curto, testar longo: atenÃ§Ã£o com polarizaÃ§Ãµes lineares permite extrapolaÃ§Ã£o de comprimento de entrada](https://arxiv.org/abs/2108.12409)ğŸ“„
-   [Primer: Procurando Transformadores Eficientes para Modelagem de Linguagem](https://arxiv.org/abs/2109.08668)ğŸ“„
-   [QuÃ£o familiar isso parece? Representacional MultilÃ­ngue
    AnÃ¡lise de similaridade de incorporaÃ§Ãµes acÃºsticas de palavras](https://arxiv.org/pdf/2109.10179.pdf)ğŸ”Š
-   [SimCSE: Aprendizagem contrastiva simples de incorporaÃ§Ãµes de frases](https://arxiv.org/abs/2104.08821#)ğŸ“„
-   [AtenÃ§Ã£o Composicional: DesembaraÃ§ando Pesquisa e RecuperaÃ§Ã£o](https://arxiv.org/abs/2110.09419)ğŸ“„ğŸ“·
-   [SPANN: pesquisa aproximada de vizinho mais prÃ³ximo em escala de bilhÃµes de dÃ³lares altamente eficiente](https://arxiv.org/abs/2111.08566)
-   [GPL: Pseudo-rotulagem generativa para adaptaÃ§Ã£o de domÃ­nio nÃ£o supervisionado de recuperaÃ§Ã£o densa](https://arxiv.org/abs/2112.07577) ğŸ“„
-   [Mecanismos de pesquisa generativos: experimentos iniciais](https://computationalcreativity.net/iccc21/wp-content/uploads/2021/09/ICCC_2021_paper_50.pdf) ğŸ“·
-   [Repensando a pesquisa: transformando diletantes em especialistas em domÃ­nio](https://dl.acm.org/doi/10.1145/3476415.3476428) -[WhiteningBERT: uma abordagem fÃ¡cil de incorporaÃ§Ã£o de frases nÃ£o supervisionadas](https://arxiv.org/abs/2104.01767)

### 2022

-   [IncorporaÃ§Ãµes de texto e cÃ³digo por prÃ©-treinamento contrastivo](https://arxiv.org/abs/2201.10005)ğŸ“„
-   [RELIC: Recuperando evidÃªncias para reivindicaÃ§Ãµes literÃ¡rias](https://arxiv.org/abs/2203.10053)ğŸ“„
-   [Trans-Encoder: modelagem nÃ£o supervisionada de pares de frases por meio de destilaÃ§Ãµes prÃ³prias e mÃºtuas](https://arxiv.org/abs/2109.13059)ğŸ“„
-   [SAMU-XLSR: RepresentaÃ§Ã£o de fala interlingual em nÃ­vel de expressÃ£o multimodal semanticamente alinhada](https://arxiv.org/abs/2205.08180)ğŸ”Š
-   [Uma anÃ¡lise de funÃ§Ãµes de fusÃ£o para recuperaÃ§Ã£o hÃ­brida](https://arxiv.org/abs/2210.11934)ğŸ“„
-   [DetecÃ§Ã£o fora de distribuiÃ§Ã£o com vizinhos mais prÃ³ximos](https://arxiv.org/abs/2204.06507)
-   [ESB: uma referÃªncia para reconhecimento de fala ponta a ponta em vÃ¡rios domÃ­nios](https://arxiv.org/abs/2210.13352)ğŸ”Š
-   [Analisando incorporaÃ§Ãµes de palavras acÃºsticas a partir de modelos de fala auto-supervisionados prÃ©-treinados](https://arxiv.org/pdf/2210.16043.pdf))ğŸ”Š
-   [Repensando com recuperaÃ§Ã£o: inferÃªncia fiel do modelo de linguagem grande](https://arxiv.org/abs/2301.00303)ğŸ“„
-   [RecuperaÃ§Ã£o densa precisa de tiro zero sem rÃ³tulos de relevÃ¢ncia](https://arxiv.org/pdf/2212.10496.pdf)ğŸ“„
-   [MemÃ³ria do transformador como Ã­ndice de pesquisa diferenciÃ¡vel](https://arxiv.org/abs/2202.06991)ğŸ“„

### 2023

-   [FINGER: InferÃªncia rÃ¡pida para pesquisa aproximada de vizinho mais prÃ³ximo baseada em grÃ¡fico](https://dl.acm.org/doi/10.1145/3543507.3583318)ğŸ“„
-   [ClassificaÃ§Ã£o de texto de â€œbaixos recursosâ€: um mÃ©todo de classificaÃ§Ã£o sem parÃ¢metros com compressores](https://aclanthology.org/2023.findings-acl.426/)ğŸ“„
-   [SparseEmbed: aprendendo representaÃ§Ãµes lexicais esparsas com incorporaÃ§Ãµes contextuais para recuperaÃ§Ã£o](https://dl.acm.org/doi/pdf/10.1145/3539618.3592065) ğŸ“„

## Artigos

-   [Combatendo a pesquisa semÃ¢ntica](https://adityamalte.substack.com/p/tackle-semantic-search/)
-   [Pesquisa semÃ¢ntica no Azure Cognitive Search](https://docs.microsoft.com/en-us/azure/search/semantic-search-overview)
-   [Como usamos a pesquisa semÃ¢ntica para tornar nossa pesquisa 10 vezes mais inteligente](https://zilliz.com/blog/How-we-used-semantic-search-to-make-our-search-10-x-smarter/)
-   [Stanford AI Blog: Construindo modelos de PNL escalÃ¡veis, explicÃ¡veis â€‹â€‹e adaptativos com recuperaÃ§Ã£o](https://ai.stanford.edu/blog/retrieval-based-NLP/)
-   [Construindo um mecanismo de pesquisa semÃ¢ntico com embeddings de palavras de espaÃ§o duplo](https://m.mage.ai/building-a-semantic-search-engine-with-dual-space-word-embeddings-f5a596eb6d90)
-   [Pesquisa de similaridade semÃ¢ntica em escala de bilhÃµes com FAISS+SBERT](https://towardsdatascience.com/billion-scale-semantic-similarity-search-with-faiss-sbert-c845614962e2)
-   [Algumas observaÃ§Ãµes sobre limites de pesquisa de similaridade](https://greglandrum.github.io/rdkit-blog/similarity/reference/2021/05/26/similarity-threshold-observations1.html)
-   [Pesquisa de imagens quase duplicadas usando hash sensÃ­vel Ã  localidade](https://keras.io/examples/vision/near_dup_search/)
-   [Curso gratuito sobre pesquisa de similaridade vetorial e Faiss](https://link.medium.com/HtFoFKlKvkb)
-   [Guia abrangente para algoritmos aproximados de vizinhos mais prÃ³ximos](https://link.medium.com/V62Z8drvEkb)
-   [Apresentando o Ã­ndice hÃ­brido para permitir a pesquisa semÃ¢ntica com reconhecimento de palavras-chave](https://www.pinecone.io/learn/hybrid-search/?utm_medium=email&_hsmi=0&_hsenc=p2ANqtz--zLu9hiyh-y_XTa7FCEpi8JESJKmif5dhpYtAxTWka8PIttaTOGE21LMZlg9EOZyPYpCm6GDvYy57tlGRwH6TjgLCsJg&utm_content=231741722&utm_source=hs_email)
-   [Pesquisa SemÃ¢ntica Argilla](https://docs.argilla.io/en/latest/guides/features/semantic-search.html)
-   [Co: aqui estÃ¡ o modelo de compreensÃ£o de texto multilÃ­ngue](https://txt.cohere.ai/multilingual/)
-   [Simplifique a pesquisa com modelos de incorporaÃ§Ã£o multilÃ­ngue](https://blog.vespa.ai/simplify-search-with-multilingual-embeddings/)

## Bibliotecas e ferramentas

-   [fastText](https://fasttext.cc/)
-   [Codificador de frase universal](https://tfhub.dev/google/universal-sentence-encoder/4)
-   [SBERT](https://www.sbert.net/)
-   [ELECTRA](https://github.com/google-research/electra)
-   [LaBSE](https://tfhub.dev/google/LaBSE/2)
-   [LASER](https://github.com/facebookresearch/LASER)
-   [Relevance AI - Plataforma vetorial da experimentaÃ§Ã£o Ã  implantaÃ§Ã£o](https://relevance.ai)
-   [Palheiro](https://github.com/deepset-ai/haystack/)
-   [Jina.AI](https://jina.ai/)
-   [pinha](https://www.pinecone.io/)
-   [Kit de ferramentas SentEval](https://github.com/facebookresearch/SentEval?utm_source=catalyzex.com)
-   [ranx](https://github.com/AmenRa/ranx)
-   [BEIR: Comparativo de RI](https://github.com/UKPLab/beir)
-   [RELiC: recuperando evidÃªncias para conjunto de dados de reivindicaÃ§Ãµes literÃ¡rias](https://relic.cs.umass.edu/)
-   [matchzoo-py](https://github.com/NTMC-Community/MatchZoo-py)
-   [deep_text_matching](https://github.com/wangle1218/deep_text_matching)
-   [Qual quadro?](http://qualframe.com/)
-   [lexica.art](https://lexica.art/)
-   [pesquisa semÃ¢ntica de emoji](https://github.com/lilianweng/emoji-semantic-search)
-   [PySerini](https://github.com/castorini/pyserini)
-   [BERTSerini](https://github.com/rsvp-ai/bertserini)
-   [BERTSimilarity](https://github.com/Brokenwind/BertSimilarity)
-   [milvus](https://www.milvus.io/)
-   [NeuroNLP++](https://plusplus.neuronlp.fruitflybrain.org/)
-   [weaviate](https://github.com/semi-technologies/weaviate)
-   [pesquisa semÃ¢ntica atravÃ©s da wikipedia-com-weaviate](https://github.com/semi-technologies/semantic-search-through-wikipedia-with-weaviate)
-   [pesquisa em linguagem natural do YouTube](https://github.com/haltakov/linguagemnatural-youtube-search)
-   [same.energy](https://www.same.energy/about)
-   [ann benchmarks](http://ann-benchmarks.com/)
-   [scaNN](https://github.com/google-research/google-research/tree/master/scann)
-   [REALM](https://github.com/google-research/linguagem/tree/master/idioma/realm)
-   [irritante](https://github.com/spotify/annoy)
-   [pynndescente](https://github.com/lmcinnes/pynndescente)
-   [nsg](https://github.com/ZJULearning/nsg)
-   [FALCONN](https://github.com/FALCONN-LIB/FALCONN)
-   [redis HNSW](https://github.com/zhao-lang/redis_hnsw)
-   [autofaiss](https://github.com/criteo/autofaiss)
-   [DPR](https://github.com/facebookresearch/DPR)
-   [rank_BM25](https://github.com/dorianbrown/rank_bm25)
-   [nearPy](http://pixelogik.github.io/NearPy/)
-   [vearch](https://github.com/vearch/vearch)
-   [vespa](https://github.com/vespa-engine/vespa)
-   [PyNNDescent](https://github.com/lmcinnes/pynndescent)
-   [pgANN](https://github.com/netrasys/pgANN)
-   [SemelhanÃ§a do Tensorflow](https://github.com/tensorflow/similarity)
-   [opensemanticsearch.org](https://www.opensemanticsearch.org/)
-   [Pesquisa SemÃ¢ntica GPT3](https://gpt3demo.com/category/semantic-search)
-   [pesquisar](https://github.com/lubianat/searchy)
-   [txtai](https://github.com/neuml/txtai)
-   [HyperTag](https://github.com/Ravn-Tech/HyperTag)
-   [vetorai](https://github.com/vector-ai/vectorai)
-   [embeddinghub](https://github.com/featureform/embeddinghub)
-   [AquilaDb](https://github.com/Aquila-Network/AquilaDB)
-   [STripNet](https://github.com/stephenleo/stripnet)

## Conjuntos de dados

-   [Hub de conjunto de dados de similaridade de texto semÃ¢ntico](https://github.com/brmson/dataset-sts)
-   [Desafio de similaridade de imagens de IA do Facebook](https://www.drivendata.org/competitions/79/competition-image-similarity-1-dev/?fbclid=IwAR31vRV0EdxRdrxtPy12neZtBJQ0H9qdLHm8Wl2DjHY09PtQdn1nEEIJVUo)
-   [WIT: conjunto de dados de texto de imagem baseado na WikipÃ©dia](https://github.com/google-research-datasets/wit)
-   [BEIR](https://github.com/beir-cellar/beir)
-   MTEB

## Conquistas

DÃª uma olhada no [quadro do projeto](https://github.com/Agrover112/awesome-semantic-search/projects/1) para ver a lista de tarefas para contribuir com qualquer uma das questÃµes em aberto.
