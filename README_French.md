# Awesome Semantic-Search [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)  [![Conventional Commits](https://img.shields.io/badge/Conventional%20Commits-1.0.0-yellow.svg)](https://conventionalcommits.org)



<img src ="logo.svg" />


Logo rÃ©alisÃ© par [@createdbytango](https://instagram.com/createdbytango). 

Le rÃ©fÃ©rentiel suivant vise Ã  servir de mÃ©ta-rÃ©fÃ©rentiel pour [Recherche sÃ©mantique](https://en.wikipedia.org/wiki/Semantic_search) et [SimilaritÃ© sÃ©mantique](http://nlpprogress.com/english/semantic_textual_similarity.html) tÃ¢ches connexes.

La recherche sÃ©mantique ne se limite pas au texte! Cela peut Ãªtre fait avec des images, de la parole, etc. Il existe donc de nombreux cas d'utilisation et applications diffÃ©rents de la recherche sÃ©mantique.

## Contenu

- [Papiers](#papers)
    - [2014](#2014)
    - [2015](#2015)
    - [2016](#2016)
    - [2017](#2017)
    - [2018](#2018)
    - [2019](#2019)
    - [2020](#2020)
    - [2021](#2021)
- [Des articles](#articles)
- [BibliothÃ¨ques et outils](#libraries-and-tools)
- [Ensembles de donnÃ©es](#datasets)
- [Ã‰tapes importantes](#milestones)

## Papiers

### 2010
- [Arbres prioritaires](https://arxiv.org/abs/1009.3527)

### 2014 
- [Un modÃ¨le sÃ©mantique latent avec mise en commun convolutive
Structure pour la recherche d'informations](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/cikm2014_cdssm_final.pdf) ğŸ“„

### 2015
- [Vecteurs de saut de pensÃ©e](https://arxiv.org/pdf/1506.06726.pdf) ğŸ“„
- [LSH pratique et optimal pour la distance angulaire](https://proceedings.neurips.cc/paper/2015/hash/2823f4797102ce1a1aec05359cc16dd9-Abstract.html)

### 2016
- [Sac d'astuces pour une classification de texte efficace](https://arxiv.org/abs/1607.01759) ğŸ“„
- [Enrichir les vecteurs de mots avec des informations sur les sous-mots](https://arxiv.org/abs/1607.04606) ğŸ“„
- [Recherche approximative du voisin le plus proche efficace et robuste Ã  l'aide de graphiques Hierarchical Navigable Small World](https://arxiv.org/abs/1603.09320)
- [Sur la recherche approximative d'incorporations de mots similaires](https://www.aclweb.org/anthology/P16-1214.pdf) 
- [Apprendre des reprÃ©sentations distribuÃ©es de phrases Ã  partir de donnÃ©es non Ã©tiquetÃ©es](https://arxiv.org/abs/1602.03483)ğŸ“„
- [Recherche approximative du voisin le plus proche sur des donnÃ©es de grande dimension --- ExpÃ©riences, analyses et amÃ©liorations](https://arxiv.org/abs/1610.02455)

### 2017
- [Supervised Learning of Universal Sentence Representations from Natural Language Inference Data](https://research.fb.com/wp-content/uploads/2017/09/emnlp2017.pdf) ğŸ“„

### 2018
- [Encodeur de phrases universel](https://arxiv.org/pdf/1803.11175.pdf) ğŸ“„
- [Apprendre la similaritÃ© textuelle sÃ©mantique Ã  partir de conversations](https://arxiv.org/pdf/1804.07754.pdf) ğŸ“„
- [Google AI Blog : AvancÃ©es en matiÃ¨re de similaritÃ© textuelle sÃ©mantique](https://ai.googleblog.com/2018/05/advances-in-semantic-textual-similarity.html) ğŸ“„
- [Optimisation de l'indexation basÃ©e sur le graphique du k-plus proche voisin pour la recherche de proximitÃ© dans les donnÃ©es de grande dimension](https://arxiv.org/abs/1810.07355)
- [Recherche rapide approximative du voisin le plus proche avec le Navigation dans le graphique d'Ã©talement](http://www.vldb.org/pvldb/vol12/p461-fu.pdf)
- [Le cas des structures d'index apprises](https://dl.acm.org/doi/10.1145/3183713.3196909)

### 2019
- [LASERÂ : reprÃ©sentations de phrases indÃ©pendantes du langage](https://engineering.fb.com/2019/01/22/ai-research/laser-multilingual-sentence-embeddings/) ğŸ“„
- [Extension de document par prÃ©diction de requÃªte](https://arxiv.org/abs/1904.08375) ğŸ“„
- [Sentence-BERTÂ : incorporations de phrases utilisant les rÃ©seaux BERT siamois](https://arxiv.org/pdf/1908.10084.pdf) ğŸ“„
- [Classement des documents en plusieurs Ã©tapes avec BERT](https://arxiv.org/abs/1910.14424) ğŸ“„
- [RÃ©cupÃ©ration latente pour la rÃ©ponse aux questions de domaine ouvert faiblement supervisÃ©](https://arxiv.org/abs/1906.00300)
- [RÃ©ponse aux questions de bout en bout en domaine ouvert avec BERTserini](https://www.aclweb.org/anthology/N19-4013/)
- [BioBERTÂ : un modÃ¨le de reprÃ©sentation du langage biomÃ©dical prÃ©-entraÃ®nÃ© pour l'exploration de textes biomÃ©dicaux](https://arxiv.org/abs/1901.08746)ğŸ“„
- [Analyser et amÃ©liorer les reprÃ©sentations avec la perte douce du plus proche voisin](https://arxiv.org/pdf/1902.01889.pdf)ğŸ“·
- [DiskANNÂ : rapide et prÃ©cis au milliard de points le plus proche Recherche de voisins sur un seul nÅ“ud](https://proceedings.neurips.cc/paper/2019/file/09853c7fb1d3f8ee67a61b6bf4a7f8e6-Paper.pdf)

### 2020
- [DÃ©ploiement rapide d'un moteur de recherche neuronal pour l'ensemble de donnÃ©es de recherche ouvert COVID-19Â : rÃ©flexions prÃ©liminaires et leÃ§ons apprises](https://arxiv.org/abs/2004.05125) ğŸ“„
- [PASSAGE RECLASSEMENT AVEC BERT](https://arxiv.org/pdf/1901.04085.pdf) ğŸ“„
- [CO-SearchÂ :Â RÃ©cupÃ©ration d'informations COVID-19 avec recherche sÃ©mantique, rÃ©ponse aux questions et rÃ©sumÃ© abstrait](https://arxiv.org/pdf/2006.09595.pdf) ğŸ“„
- [LaBSEÂ : intÃ©gration de phrases BERT indÃ©pendante de la langue](https://arxiv.org/abs/2007.01852) ğŸ“„
- [CovidexÂ : modÃ¨les de classement neuronal et infrastructure de recherche de mots clÃ©s pour l'ensemble de donnÃ©es de recherche ouvert COVID-19](https://arxiv.org/abs/2007.07846) ğŸ“„
- [DeTectÂ : un framework NLP approfondi pour une comprÃ©hension de texte intelligente](https://engineering.linkedin.com/blog/2020/open-sourcing-detext) ğŸ“„
- [Faire des inclusions de phrases unilingues multilingues Ã  l'aide de la distillation des connaissances](https://arxiv.org/pdf/2004.09813.pdf) ğŸ“„
- [Transformateurs prÃ©formÃ©s pour le classement des textesÂ : BERT et au-delÃ ](https://arxiv.org/abs/2010.06467) ğŸ“„
- [REALMÂ :Â prÃ©-formation sur le modÃ¨le de langage amÃ©liorÃ© par la rÃ©cupÃ©ration](https://arxiv.org/abs/2002.08909)
- [ELECTRA : LES ENCODEURS DE TEXTE DE PRÃ‰-FORMATION COMME DES DISCRIMINATEURS PLUTT QUE DES GÃ‰NÃ‰RATEURS](https://openreview.net/pdf?id=r1xMH1BtvB)ğŸ“„
- [AmÃ©liorer l'apprentissage en profondeur pour la recherche Airbnb](https://arxiv.org/pdf/2002.05515)
- [GÃ©rer la diversitÃ© dans la recherche Airbnb](https://arxiv.org/abs/2004.02621)ğŸ“„
- [Apprentissage par contraste nÃ©gatif approximatif du voisin le plus proche pour une recherche de texte dense](https://arxiv.org/abs/2007.00808v1)ğŸ“„
### 2021
- [Approche hybride pour le calcul de similaritÃ© sÃ©mantique entre les mots tamouls](https://www.researchgate.net/publication/350112163_Hybrid_approach_for_semantic_similarity_calculation_between_Tamil_words) ğŸ“„
- [SBERT augmentÃ©](https://arxiv.org/pdf/2010.08240.pdf) ğŸ“„
- [BEIRÂ : une rÃ©fÃ©rence hÃ©tÃ©rogÃ¨ne pour l'Ã©valuation Ã  zÃ©ro des modÃ¨les de recherche d'informations](https://arxiv.org/abs/2104.08663) ğŸ“„
- [Recherche visuelle hÃ©tÃ©rogÃ¨ne tenant compte de la compatibilitÃ©](https://arxiv.org/abs/2105.06047) ğŸ“·
- [Apprendre le style personnel Ã  partir de quelques exemples](https://chuanenlin.com/personalstyle)ğŸ“·
- [TSDAEÂ : Utilisation de l'encodeur automatique Ã  dÃ©bruitage sÃ©quentiel basÃ© sur un transformateur pour l'apprentissage par incorporation de phrases non supervisÃ©](https://arxiv.org/abs/2104.06979)ğŸ“„
- [Une enquÃªte sur les transformateurs](https://arxiv.org/abs/2106.04554)ğŸ“„ğŸ“·
- [Suggestions de requÃªtes de recherche associÃ©es de haute qualitÃ© Ã  l'aide de l'apprentissage par renforcement approfondi](https://arxiv.org/abs/2108.04452v1)
- [RÃ©cupÃ©ration de produits basÃ©e sur l'intÃ©gration dans la recherche Taobao](https://arxiv.org/pdf/2106.09297.pdf)ğŸ“„ğŸ“·
- [TPRMÂ : un modÃ¨le de classement personnalisÃ© basÃ© sur des sujets pour la recherche sur le Web](https://arxiv.org/abs/2108.06014)ğŸ“„
- [mMARCOÂ : une version multilingue de l'ensemble de donnÃ©es de classement de passage MSMARCO](https://arxiv.org/abs/2108.13897)ğŸ“„
- [Raisonnement de base de donnÃ©es sur le texte](https://aclanthology.org/2021.acl-long.241.pdf)ğŸ“„
- [Comment le rÃ©glage fin accusatoire profite-t-il Ã  BERTÂ ?](https://arxiv.org/abs/2108.13602))ğŸ“„
- [Train Short, Test LongÂ : l'attention avec les biais linÃ©aires permet l'extrapolation de la longueur d'entrÃ©e](https://arxiv.org/abs/2108.12409)ğŸ“„
- [IntroductionÂ : Ã€ la recherche de transformateurs efficaces pour la modÃ©lisation du langage](https://arxiv.org/abs/2109.08668)ğŸ“„
- [Ã€ quel point cela semble-t-il familierÂ ? ReprÃ©sentation multilingue Analyse de similaritÃ© des plongements de mots acoustiques](https://arxiv.org/pdf/2109.10179.pdf)ğŸ”Š
- [SimCSE : Apprentissage Contrastif Simple des Incorporations de Phrases](https://arxiv.org/abs/2104.08821#)ğŸ“„
- [Attention compositionnelleÂ : dÃ©mÃªler la recherche et la rÃ©cupÃ©ration](https://arxiv.org/abs/2110.09419)ğŸ“„ğŸ“·
- [SPANNÂ : recherche trÃ¨s efficace du voisin le plus proche Ã  l'Ã©chelle d'un milliard](https://arxiv.org/abs/2111.08566)

## Des articles
- [S'attaquer Ã  la recherche sÃ©mantique](https://adityamalte.substack.com/p/tackle-semantic-search/)
- [Recherche sÃ©mantique dans Azure Congnitive Search](https://docs.microsoft.com/en-us/azure/search/semantic-search-overview)
- [Comment nous avons utilisÃ© la recherche sÃ©mantique pour rendre notre recherche 10 fois plus intelligente](https://zilliz.com/blog/How-we-used-semantic-search-to-make-our-search-10-x-smarter/)
- [Construire un moteur de recherche sÃ©mantique avec des inclusions de mots Ã  double espace](https://m.mage.ai/building-a-semantic-search-engine-with-dual-space-word-embeddings-f5a596eb6d90)
- [Recherche de similaritÃ© sÃ©mantique Ã  l'Ã©chelle d'un milliard avec FAISS+SBERT](https://towardsdatascience.com/billion-scale-semantic-similarity-search-with-faiss-sbert-c845614962e2)
- [Quelques observations sur les seuils de recherche de similaritÃ©](https://greglandrum.github.io/rdkit-blog/similarity/reference/2021/05/26/similarity-threshold-observations1.html)
- [Recherche d'images presque en double Ã  l'aide du hachage sensible Ã  la localitÃ©](https://keras.io/examples/vision/near_dup_search/)
- [Cours gratuit sur la recherche de similaritÃ© vectorielle et Faiss](https://link.medium.com/HtFoFKlKvkb)
- [Guide complet des algorithmes approximatifs des voisins les plus proches](https://link.medium.com/V62Z8drvEkb)
## BibliothÃ¨ques et outils
- [fastText](https://fasttext.cc/)
- [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/4)
- [SBERT](https://www.sbert.net/)
- [ELECTRA](https://github.com/google-research/electra)
- [LaBSE](https://tfhub.dev/google/LaBSE/2)
- [LASER](https://github.com/facebookresearch/LASER)
- [Haystack](https://github.com/deepset-ai/haystack/)
- [Jina.AI](https://jina.ai/)
- [SentEval Toolkit](https://github.com/facebookresearch/SentEval?utm_source=catalyzex.com)
- [BEIR :Benchmarking IR](https://github.com/UKPLab/beir)
- [matchzoo-py](https://github.com/NTMC-Community/MatchZoo-py)
- [deep_text_matching](https://github.com/wangle1218/deep_text_matching)
- [Which Frame?](http://whichframe.com/)
- [PySerini](https://github.com/castorini/pyserini)
- [BERTSerini](https://github.com/rsvp-ai/bertserini)
- [BERTSimilarity](https://github.com/Brokenwind/BertSimilarity)
- [milvus](https://www.milvus.io/)
- [weaviate](https://github.com/semi-technologies/weaviate)
- [natural-language-youtube-search](https://github.com/haltakov/natural-language-youtube-search)
- [same.energy](https://www.same.energy/about)
- [scaNN](https://github.com/google-research/google-research/tree/master/scann)
- [REALM](https://github.com/google-research/language/tree/master/language/realm)
- [annoy](https://github.com/spotify/annoy)
- [nsg](https://github.com/ZJULearning/nsg)
- [FALCONN](https://github.com/FALCONN-LIB/FALCONN)
- [redis HNSW](https://github.com/zhao-lang/redis_hnsw)
- [autofaiss](https://github.com/criteo/autofaiss)
- [DPR](https://github.com/facebookresearch/DPR)
- [rank_BM25](https://github.com/dorianbrown/rank_bm25)
- [nearPy](http://pixelogik.github.io/NearPy/)
- [vearch](https://github.com/vearch/vearch)
- [PyNNDescent](https://github.com/lmcinnes/pynndescent)
- [pgANN](https://github.com/netrasys/pgANN)
- [Tensorflow Similarity](https://github.com/tensorflow/similarity)
- [opensemanticsearch.org](https://www.opensemanticsearch.org/)
- [GPT3 Semantic Search](https://gpt3demo.com/category/semantic-search)
- [searchy](https://github.com/lubianat/searchy)
- [txtai](https://github.com/neuml/txtai)
- [HyperTag](https://github.com/Ravn-Tech/HyperTag)
- [vectorai](https://github.com/vector-ai/vectorai)
- [embeddinghub](https://github.com/featureform/embeddinghub)
- [AquilaDb](https://github.com/Aquila-Network/AquilaDB)
## Ensembles de donnÃ©es
- [Hub de jeu de donnÃ©es de similaritÃ© de texte sÃ©mantique](https://github.com/brmson/dataset-sts)
- [DÃ©fi de similaritÃ© d'image Facebook AI](https://www.drivendata.org/competitions/79/competition-image-similarity-1-dev/?fbclid=IwAR31vRV0EdxRdrxtPy12neZtBJQ0H9qdLHm8Wl2DjHY09PtQdn1nEEIJVUo)
- [WITÂ : Ensemble de donnÃ©es de texte d'image basÃ© sur WikipÃ©dia](https://github.com/google-research-datasets/wit)
## Ã‰tapes importantes

Jetez un Å“il au [comitÃ© de projet](https://github.com/Agrover112/awesome-semantic-search/projects/1) pour que la liste des tÃ¢ches contribue Ã  l'un des problÃ¨mes ouverts.
